image:
  repository: gcr.io/google-containers/fluentd-elasticsearch
  tag: v2.2.0
  pullPolicy: IfNotPresent

resources: {}
  # limits:
  #   cpu: 100m
  #   memory: 500Mi
  # requests:
  #   cpu: 100m
  #   memory: 200Mi

elasticsearch:
  host: 'elasticsearch-logging-client.kintohub.svc.cluster.local'
  port: 9200
  buffer_chunk_limit: 2M
  buffer_queue_limit: 8

rbac:
  create: true

serviceAccount:
  create: true

livenessProbe:
  enabled: true

service:
  type: ClusterIP
  ports:
    - name: "fluentd-agent"
      port: 24224

tolerations:
- key: "type"
  operator: "Equal"
  value: "reserved"
  effect: "NoSchedule"

configMaps:
  system.conf: |-
    <system>
      root_dir /tmp/fluentd-buffers/
    </system>
  containers.input.conf: |-
    <source>
      @id fluentd-json-svls-containers.log
      @type tail
      path /var/log/containers/*svls*.log
      pos_file /var/log/fluentd-json-svls-containers.log.pos
      time_format %Y-%m-%dT%H:%M:%S.%NZ
      tag raw.kinto.json.svls.*
      read_from_head true
      format json
    </source>
    <source>
      @id fluentd-json-gw-containers.log
      @type tail
      path /var/log/containers/*gateway-middleware*.log
      pos_file /var/log/fluentd-json-gw-containers.log.pos
      time_format %Y-%m-%dT%H:%M:%S.%NZ
      tag raw.kinto.json.gw.*
      read_from_head true
      format json
    </source>
    <source>
      @id fluentd-svls-label-containers.log
      @type tail
      path /var/log/containers/*svls*.log
      pos_file /var/log/fluentd-svls-label-containers.log.pos
      time_format %Y-%m-%dT%H:%M:%S.%NZ
      tag raw.kinto.svls.label.*
      read_from_head true
      format json
    </source>
    <source>
      @id fluentd-argo-build-containers.log
      @type tail
      path /var/log/containers/*argo-build*.log
      pos_file /var/log/fluentd-argo-build-containers.log.pos
      time_format %Y-%m-%dT%H:%M:%S.%NZ
      tag raw.kinto.argo.build.*
      read_from_head true
      format json
    </source>
    # Detect exceptions in the log output and forward them as one log entry.
    <match raw.kinto.**>
      @id raw.kinto
      @type detect_exceptions
      remove_tag_prefix raw
      message log
      stream stream
      multiline_flush_interval 5
      max_bytes 500000
      max_lines 1000
    </match>
  forward.input.conf: |-
    # Takes the messages sent over TCP
    <source>
      @type forward
      port 24224
      bind 0.0.0.0
    </source>
  output.conf: |
    # Enriches records with Kubernetes metadata
    <filter kinto.**>
      @type kubernetes_metadata
      cache_ttl 1
      watch true
    </filter>

    <filter kinto.json.**>
      @type parser
      key_name log
      reserve_data true
      inject_key_prefix kinto-
      <parse>
        @type json
      </parse>
    </filter>

    <filter kinto.json.**>
      @type record_transformer
      enable_ruby true
      <record>
        kinto-workspaceid ${record["kinto-kinto_request_id"].split('-')[0]}
        kinto-applicationid ${record["kinto-kinto_request_id"].split('-')[1]}
        kinto-requestid ${record["kinto-kinto_request_id"].split('-')[2]}
      </record>
    </filter>

    <filter kinto.json.svls.**>
      @type record_transformer
      enable_ruby true
      remove_keys kinto-kinto_request_id
      <record>
        target_index request-svls-${record["kinto-workspaceid"]}-${time.strftime("%Y.%m.%d")}
      </record>
    </filter>

    <filter kinto.json.gw.**>
      @type record_transformer
      enable_ruby true
      remove_keys kinto-kinto_request_id
      <record>
        target_index request-${record["kinto-workspaceid"]}-${time.strftime("%Y.%m.%d")}
      </record>
    </filter>

    <filter kinto.argo.build.**>
      @type record_transformer
      enable_ruby true
      <record>
        target_index build-${record["kubernetes"]["labels"]["workspace"]}-${time.strftime("%Y.%m.%d")}
      </record>
    </filter>

    <filter kinto.svls.label.**>
      @type record_transformer
      enable_ruby true
      <record>
        target_index svls-${record["kubernetes"]["labels"]["workspace"]}-${time.strftime("%Y.%m.%d")}
      </record>
    </filter>

    <filter kinto.**>
      @type record_transformer
      enable_ruby true
      <record>
        nanoseconds ${time.strftime("%N")}
      </record>
    </filter>

    <match kinto.**>
      @type copy
      <store>
        @id elasticsearch_kubernetes
        @type elasticsearch
        @log_level info
        include_tag_key true
        host "#{ENV['OUTPUT_HOST']}"
        port "#{ENV['OUTPUT_PORT']}"
        logstash_format true
        target_index_key target_index
        index_name kinto-fallback
        <buffer>
          @type file
          path /var/log/fluentd-buffers/kubernetes.system.buffer
          flush_mode interval
          retry_type exponential_backoff
          flush_thread_count 2
          flush_interval 5s
          retry_forever
          retry_max_interval 30
          chunk_limit_size "#{ENV['OUTPUT_BUFFER_CHUNK_LIMIT']}"
          queue_limit_length "#{ENV['OUTPUT_BUFFER_QUEUE_LIMIT']}"
          overflow_action block
        </buffer>
      </store>
      # <store>
      #   @type stdout
      # </store>
    </match>

    # # source forward - log sent by istio directly
    # <match **.istio-system>
    #   @id elasticsearch_istio
    #   @type elasticsearch
    #   @log_level info
    #   include_tag_key true
    #   host "#{ENV['OUTPUT_HOST']}"
    #   port "#{ENV['OUTPUT_PORT']}"
    #   logstash_format true
    #   logstash_prefix kinto.istio
    #   <buffer>
    #     @type file
    #     path /var/log/fluentd-buffers/istio.system.buffer
    #     flush_mode interval
    #     retry_type exponential_backoff
    #     flush_thread_count 2
    #     flush_interval 5s
    #     retry_forever
    #     retry_max_interval 30
    #     chunk_limit_size "#{ENV['OUTPUT_BUFFER_CHUNK_LIMIT']}"
    #     queue_limit_length "#{ENV['OUTPUT_BUFFER_QUEUE_LIMIT']}"
    #     overflow_action block
    #   </buffer>
    # </match>

    # Throw away anything else
    <match **>
      @type null
    </match>
